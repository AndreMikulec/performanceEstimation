\documentclass[10pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{fancyvrb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}

\newcommand{\PE}{package \texttt{performanceEstimation}\ }

\author{Luis Torgo\\FCUP - LIAAD/INESC Tec\\University of Porto\\
  \texttt{ltorgo@dcc.fc.up.pt}, \texttt{ltorgo@inescporto.pt}}
\title{An Infra-Structure for Performance Estimation\\ and Experimental Comparison of Predictive Models}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\begin{document}




%$
\maketitle

\begin{abstract}
  
  This document describes an infra-structure provided by the R \PE
  that allows to estimate the predictive performance of different
  approaches (workflows) to predictive tasks.  The infra-structure is
  generic in the sense that it can be used to estimate the values of
  any performance metrics, for any workflow on different predictive
  tasks, namely, classification, regression and time series tasks. The
  package also includes several standard workflows that allow users to
  easily set up their experiments limiting the amount of work and
  information they need to provide. The overall goal of the
  infra-structure provided by our package is to facilitate the task of
  estimating the predictive performance of different modeling
  approaches to predictive tasks in the R environment.
   
\end{abstract}

% ====================================================================
\section{Introduction}

The goal of this document is to describe the infra-structure that is
available in \PE to estimate the performance of different approaches
to predictive tasks.  The main goal of this package is to provide a
general infra-structure that can be used to estimate the performance
using several predictive performance metrics of any modelling approach
to different predictive tasks, with a minimal effort from the
user. The package provides this type of facilities for classification,
regression and time series tasks. There is no limitation on the type
of approaches to these tasks for which you can estimate the
performance - the user just needs to provide a workflow function
(following some interface rules) that implements the approach for
which the predictive performance is to be estimated. The package
includes some standard workflow functions that implement the standard
learn+test+evaluate approach that most users will be interested
in. This means that if you just want to estimate the performance of
some variants of a method already implemented in R (e.g. an SVM), on
some particular tasks, you will be able to use these standard workflow
functions and thus your input will be limited to the minimum. The
package also provides a series of predictive performance metrics for
different tasks. Still, you are not limited to these metrics and can
use any metric as long as it exists in R or you provide a function
calculating it.

This infra-structure implements different methods for estimating the
predictive performance. Namely, you can select among: (i) cross
validation, (ii) holdout and random subsampling, (iii) leave one out
cross validation, (iv) ($\epsilon_0$ and .631) bootstrap and also (v)
Monte-Carlo experiments for time series forecasting tasks. For each of
these tasks different options are implemented (e.g. use of stratified
sampling).

Most of the times experimental methodologies for performance estimation are iterative
processes that repeat the modelling task several times using different
data samples with the goal of improving the accuracy of the
estimates. The estimates are the result of aggregating the scores
obtained on each of the repetitions. For each of these
repetitions different training and testing samples are generated and
the process being evaluated is "asked" to: (i) obtain the predictive
model using the training data, and then (ii) use this model to obtain
predictions for the respective test sample, which in turn (iii) can be
used to calculate the scores of the performance metrics being
estimated. This means that there is a workflow that starts with a
predictive task for which training and testing samples are given, and
that it should produce as result the scores of the performance metrics
being estimated. There are far too many possible approaches and
sub-steps for the implementation of this workflow.  To ensure full
generality of the infra-structure, we ask the user to provide a
function that implements this workflow for each of the predictive
approaches he/she wishes to compare and/or evaluate. This function can
be parametrizable in the sense that there may be variants of the
workflow that the user wishes to evaluate and/or compare. Still, the
goal of this workflow implementation functions is very clear: (i)
receive as input a predictive task for which training  and  test
samples are given, as well as any eventual workflow specific parameters; and
(ii) produce as result a set of scores for the evaluation metrics
being estimated. These scores should be obtained by applying some
modelling technique to the training sample and then use the resulting
to model to obtain predictions for the test sample. These predictions
should then be used to obtain the scores of the predictive metrics for which the
user is interested in obtaining reliable estimates.

The infra-structure we describe here provides means for the user to
indicate: (i) a set of predictive tasks with the respective data sets;
(ii) a set of workflows and respective variants; and (iii) an
experimental methodology. The infra-structure then takes care of all
the process of experimentally estimating the predictive performance of the different approaches on
the tasks, producing as result an object that can be
explored in different ways to obtain the results of the estimation process.
The infra-structure also provides several utility functions
to explore these results objects, for instance to
obtain summaries of the estimation process both in textual format as well as
visually. Moreover, it also provides functions that carry out
statistical significance tests based on the outcome of the
experiments. 

Finally, the infra-structure provides utility functions
implementing frequently used workflows for common modelling techniques, as
well as functions that facilitate the automatic generation of variants
of workflows by specifying sets of parameters that the user wishes to
consider in the comparisons.

% ====================================================================
\section{A Simple Illustrative Example}\label{sec:simpleEx}

Let us assume we are interested in estimating the predictive performance of several variants of
an SVM on the
\textbf{Iris} classification problem. More specifically, we want to
obtain a reliable estimate of the error rate of these variants using
10-fold cross validation. The following code illustrates how these
estimates could be obtained with our proposed infra-structure.

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(performanceEstimation)}  \hlcom{# Loading our infra-structure}
\hlkwd{library}\hlstd{(e1071)}                  \hlcom{# A package containing SVMs}

\hlkwd{data}\hlstd{(iris)}                      \hlcom{# The data set we are going to use}

\hlstd{res} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
         \hlkwd{PredTask}\hlstd{(Species} \hlopt{~} \hlstd{.,iris),}
         \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
                          \hlkwc{learner}\hlstd{=}\hlstr{'svm'}\hlstd{,}
                          \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{cost}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{5}\hlstd{,}\hlnum{10}\hlstd{),}\hlkwc{gamma}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.1}\hlstd{,}\hlnum{0.001}\hlstd{))}
                         \hlstd{),}
         \hlkwd{CvSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{1}\hlstd{,}\hlkwc{nFolds}\hlstd{=}\hlnum{10}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


This simple example illustrates several key concepts of our
infra-structure. First of all, we have the main function  -
\texttt{performanceEstimation()}, which is used to carry out
the estimation process. It has 3 arguments: (i) a vector of
predictive tasks (in the example a single one); (ii) a vector of workflows; and (iii) the
estimation settings. 

Predictive tasks are S4 objects of class
\textbf{PredTask}. This class, also defined in our package,
describes a predictive task by: (i) a formula; (ii) the source data
set (an R data frame); and (iii) an optional name of the task (a
string). 

Workflows are S4 objects of class \textbf{Workflow}, which are also
defined within the package. These objects include two pieces
of information: (i) the name of the function (a string) implementing the
workflow; and (ii) the list of parameters to this function. The
function will be called from within 
\texttt{performanceEstimation()}  with a formula in the first
argument, a training sample (a data frame) on the second argument, a
test sample (another data frame) on the third, and then all the parameters
the user specifies in the list of parameters used when creating the
\textbf{Workflow} object. This means that the object\\
\texttt{Workflow('svmTrial',pars=list(cost=10,gamma=0.5))},\\ if used in a
call to \texttt{performanceEstimation}, will 
 generate calls of the type
\texttt{svmTrial(\textit{someFormula}, \textit{someTrainingSample}, \textit{someTestSample}, cost=10,
  gamma=0.5)}. In the illustrative example above we are using the function \\ \texttt{workflowVariants()} from our package to automatically
generate a vector of \textbf{Workflow} objects. This function can be used
to generate different variants of a workflow function using all
combinations of some parameters of the workflow. In the code
above the workflow function is \texttt{standardWF} which is another
auxiliar function we provide. This function implements a typical
workflow for different modelling techniques that are indicated through
its parameter \texttt{learner}. Above we are using it to generate
variants of calls to the \texttt{svm()} function, which is an implementation of an SVM available in package \texttt{e1071}~\cite{e1071}.  Later we will provide
more details on the \texttt{standardWF} function. For now we can think of the call
to the \texttt{workflowVariants()} function as generating the following vector:\linebreak

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{c}(\hlkwd{Workflow}(\hlstr{'standardWF'},
          pars=\hlkwd{list}(learner=\hlstr{'svm'},
                    learner.pars=\hlkwd{list}(cost=1,gamma=0.1))
         ), 
  \hlkwd{Workflow}(\hlstr{'standardWF'},
          pars=\hlkwd{list}(learner=\hlstr{'svm'},
                    learner.pars=\hlkwd{list}(cost=5,gamma=0.1))
         ),
...
...
 ) 
\end{alltt}
\end{kframe}
\end{knitrout}

 
The workflow implemented through function \texttt{standardWF()} by
default calculates the error rate of the used modelling techniques if
handling classification tasks, though we will see later that we can
specify other metrics.

Finally, the third parameter of the function
\texttt{performanceEstimation()} specifies the estimation settings
to use in the process. It is an S4 object of class
\textbf{EstimationSettings} that in effect is an union that includes among
others the S4 class \textbf{CvSettings}. Objects of this latter class
include information on the number of repetitions of the cross
validation process (in our example 1 single repetition, which is the
default), the number of folds (10 above, which is also the default),
the random number generator seed and a logical indicating whether stratified
samples should be used (defaulting to \texttt{FALSE}, i.e. no stratification).

The result of the call to \texttt{performanceEstimation()} is an S4
object of the class \textbf{ComparisonResults}. These objects tipically are not
 directly explored by the end-user so we ommit their
details here\footnote{Interested readers may have a look at the corresponding
  help page - \texttt{class?ComparisonResults} .}. There are several utility
functions that allow the users to explore the results of the
experimental comparisons. Here are a few illustrative examples:






\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
## 
## == Summary of a  Cross Validation Performance Estimation Experiment ==
##  1 x 10 - Fold Cross Validation run with seed =  1234 
## 
## * Predictive Tasks ::  iris
## * Workflows  ::  svm.v1, svm.v2, svm.v3, svm.v4, svm.v5, svm.v6
## 
## * Summary of Experiment Results:
## 
## -> Datataset:  iris 
## 
## 	*Workflow: svm.v1 
##             err
## avg     0.04667
## std     0.05488
## min     0.00000
## max     0.13333
## invalid 0.00000
## 
## 	*Workflow: svm.v2 
##             err
## avg     0.04000
## std     0.04661
## min     0.00000
## max     0.13333
## invalid 0.00000
## 
## 	*Workflow: svm.v3 
##             err
## avg     0.04000
## std     0.04661
## min     0.00000
## max     0.13333
## invalid 0.00000
## 
## 	*Workflow: svm.v4 
##            err
## avg     0.6867
## std     0.2014
## min     0.3333
## max     0.9333
## invalid 0.0000
## 
## 	*Workflow: svm.v5 
##             err
## avg     0.15333
## std     0.11780
## min     0.06667
## max     0.46667
## invalid 0.00000
## 
## 	*Workflow: svm.v6 
##             err
## avg     0.10667
## std     0.06441
## min     0.00000
## max     0.20000
## invalid 0.00000
\end{verbatim}
\end{kframe}
\end{knitrout}


The generic function \texttt{summary} allows us to obtain the
estimated scores for each compared approach on each predictive
task. For each performance metric (in this case only the error rate),
the function shows the estimated average performance, the standard
error of this estimate as well as minimum and maximum scores on the
different iterations of the experimental comparison. Moreover,
information is also given on eventual failures on some of the
iterations.

The best scores for each predictive task and metric can be obtained as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{topPerformers}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
## $iris
##     Workflow Estimate
## err   svm.v2     0.04
\end{verbatim}
\end{kframe}
\end{knitrout}


The generic function \texttt{plot} can be used to obtain a graphical
display of the distribution of performance metrics across the
different iterations of the estimation process using box-plots, as
show in Figure~\ref{fig:ex1Iris}. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(res)}
\end{alltt}
\end{kframe}\begin{figure}[]


{\centering \includegraphics[width=0.7\textwidth]{figures/perfEst-ex1Iris} 

}

\caption[The distribution of the error rate on the 10 folds]{The distribution of the error rate on the 10 folds.\label{fig:ex1Iris}}
\end{figure}


\end{knitrout}


You might have observed that the infra-structure uses some IDs to
describe each workflow variant (e.g. \texttt{svm.v1}). The user can check
the parameter configuration corresponding to some ID as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{getWorkflow}\hlstd{(}\hlstr{'svm.v1'}\hlstd{,res)}
\end{alltt}
\begin{verbatim}
## Workflow Object:
## 	Workflow ID       ::  svm.v1 
## 	Workflow Function ::  standardWF
## 		Parameter values:
## 		 learner.pars  ->  cost=1 gamma=0.1 
## 		 learner  ->  svm
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Predictive Tasks}

Predictive tasks are data analysis problems where we want to obtain a
model of an unknown function $Y=f(X_1, X_2, \cdots, X_p)$ that relates
a target variable $Y$ with a set of $p$ predictors $X_1, X_2, \cdots,
X_p$. The model is usually obtained using a sample of $n$ observations
of the mapping of the unknown function, $D=\{\langle \mathbf{x}_i,
Y_i\rangle\}_{i=1}^n$, where $\mathbf{x}_i$ is a vector with the $p$
predictors values and $Y_i$ the respective target variable value.  These data sets in R are usually stored in data
frames, and formula objects are used to specify the form of the
functional dependency that we are trying to model, i.e. which is the
target variable and the predictors.

Objects of class \textbf{PredTask} encapsulate the information of a
predictive task, i.e. the functional form and the data required for
solving it. For convenience they also allow the user to assign a name
to each task. These S4 objects can be created using the construtor
function \texttt{PredTask()}, as seen in the following example:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(iris)}
\hlkwd{PredTask}\hlstd{(Species} \hlopt{~} \hlstd{.,iris,}\hlstr{'irisClassificationTask'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Prediction Task Object:
## 	Task Name ::  irisClassificationTask
## 	Formula   :: Species ~ .
## 	Task Data ::
## 'data.frame':	150 obs. of  5 variables:
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
\end{verbatim}
\end{kframe}
\end{knitrout}


We should remark that the objects of this class only store the data
required for the specified task, as it should be clear from this other
simple example:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(iris)}
\hlkwd{PredTask}\hlstd{(Species} \hlopt{~} \hlstd{Petal.Length} \hlopt{+} \hlstd{Sepal.Length,iris,}\hlstr{'ShortIrisTask'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Prediction Task Object:
## 	Task Name ::  ShortIrisTask
## 	Formula   :: Species ~ Petal.Length + Sepal.Length
## 	Task Data ::
## 'data.frame':	150 obs. of  3 variables:
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
\end{verbatim}
\end{kframe}
\end{knitrout}


So, although we have supplied the full data frame to the constructor
function, as the task only uses 3 of the columns, the resulting
\textbf{PredTask} object only includes the information on the columns
required for this task.

% ====================================================================
\section{Workflows}

Estimation methodologies work most of the times by re-sampling the
available data set $D$ in order to create different train and test
samples from $D$ (an exception being a single repetition of hold-out). The goal is to estimate the predictive performance
of a proposed workflow to solve the task, by using these different
samples to increase our confidence on the estimates. This workflow
consists on the process of obtaining a model from a given training
sample and then use it to obtain predictions for the given test
set. This process can include several steps, e.g. specific data
pre-processing steps, and may use any modelling approach, eventually
developed  by the user. 

\subsection{User-defined Workflows}

With the goal of ensuring that the
proposed infra-structure is able to cope with all these possible usage
scenarios, we ask the user to take care of the writing of a function
implementing each workflow being compared/evaluated. These
user-defined workflow functions should be written assuming that the
first three parameters are: (i) the formula defining the predictive
task; (ii) the provided training sample; and (iii) the test sample
where to evaluate the obtained model. The functions may eventually
accept other arguments with specific parameters of the workflow. The
following is a general sketch of a user-defined workflow function:

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{myWorkFlow} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{form}\hlstd{,}\hlkwc{train}\hlstd{,}\hlkwc{test}\hlstd{,}\hlkwc{...}\hlstd{,}
                       \hlkwc{.outPreds}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{.outModel}\hlstd{=}\hlnum{TRUE}\hlstd{) \{}
  \hlkwd{require}\hlstd{(mySpecialPackage,}\hlkwc{quietly}\hlstd{=T)}
  \hlcom{## cary out some data pre-processing}
  \hlstd{myTrain} \hlkwb{<-} \hlkwd{mySpecificPreProcessingSteps}\hlstd{(train)}
  \hlcom{## now obtain the model}
  \hlstd{myModel} \hlkwb{<-} \hlkwd{myModelingTechnique}\hlstd{(form,myTrain,...)}
  \hlcom{## obtain the predictions}
  \hlstd{preds} \hlkwb{<-} \hlkwd{predict}\hlstd{(myModel,test)}
  \hlcom{## obtain the evaluation metric scores}
  \hlstd{scores} \hlkwb{<-} \hlkwd{mySpecialEvaluationMetrics}\hlstd{(}\hlkwd{responseValues}\hlstd{(form,test),preds)}
  \hlcom{## finally produce the worflow output object}
  \hlstd{res} \hlkwb{<-} \hlkwd{WFoutput}\hlstd{(scores)}
  \hlkwa{if} \hlstd{(.outPreds)}
      \hlkwd{workflowPredictions}\hlstd{(res)} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwd{responseValues}\hlstd{(form,test),}
                                       \hlstd{preds,}
                                       \hlkwd{rownames}\hlstd{(test))}
  \hlkwa{if} \hlstd{(.outModel)} \hlkwd{workflowInformation}\hlstd{(res)} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{model}\hlstd{=myModel)}
  \hlstd{res}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}



Not all workflows will require all these steps, though some may even
require more. This is clearly something that is up to the user. The
only strict requirements for these functions are: (i) the first 3 parameters
of the workflow function should be the formula, train and test data
frames; and (ii) the result of the function should be an object of the S4 class \textbf{WFoutput}.

Objects of class \textbf{WFoutput} contain the result of applying the workflow to a train+test partition. They are created using the constructor function \textbf{WFoutput()} that takes as argument a named vector with the scores of the metrics being estimated. These objects may optionally contain information on the true and predicted values for the target variable in the provided test set, as well as any other information the creator of the workflow function deems important to return. These two optional pieces of information are typically "attached" to the object using the replacement functions \textbf{workflowPredictions()}  and \textbf{workflowInformation()}, respectively. The \textbf{workflowPredictions()} function takes as value a list with at least two components. The first component is a vector with the true values of the target variable in the test set. The second component are the corresponding predictions made by the workflow. Finally, you may optionally provide a vector with the names of the rows in the test set. Regarding the \textbf{workflowInformation()} function it accepts as value also a list but whose content is completely free and left to the creator of the workflow function. Both these two functions can also be used to obtain the respective content from \textbf{WFoutput} objects, as we will see later in some of the illustrative examples.

The sketch shown above also illustrates the use of the function
\texttt{responseValues()} that can be used to obtain the values of the target
variable given a formula and a data frame.

Users should write one such workflow function for each process they
want to evaluate/compare. As mentioned before these functions may
accept further parameters on top of the 3 mandatory parameters. These
extra parameters will typically be parameters of the modelling
technique being used within the workflow but it is up to the user to
control this. As we have seen in Section~\ref{sec:simpleEx} we provide
the function \texttt{workflowVariants()} to facilitate the specification of
different variants of any workflow function by trying all combinations
of several of its specific parameters. For instance, if the modelling
function in the above example workflow (function
\texttt{myModelingTechnique()}) had an integer parameter \texttt{x}
and a Boolean parameter \texttt{y}, we could generate several
\textbf{Workflow} objects to be evaluated/compared using the
\texttt{performanceEstimation()} function, as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{workflowVariants}\hlstd{(}\hlstr{'myWorkFlow'}\hlstd{,}\hlkwc{x}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{5}\hlstd{,}\hlnum{7}\hlstd{),}\hlkwc{y}\hlstd{=}\hlkwd{c}\hlstd{(T,F))}
\end{alltt}
\end{kframe}
\end{knitrout}


This would generate 8 variants of the same workflow with all
combinations of the specified values for the 2 parameters.  This means
that any parameter of the \texttt{workflowVariants()} function that has more
than one element is assumed to be a source for generation of
variants. There may be situations where this is not intended, because
a particular argument of the workflow function is supposed to be a
vector. In these cases the user needs to ``tell'' the
\texttt{workflowVariants()} function that it should not generate variants from
the values of that parameter. Suppose that on the above example the
parameter \texttt{x} takes as values a vector, and thus your meaning
in the above statement is that you only have two variants of the
workflow (the different values of the parameter \texttt{y}). You could get
that effect by calling the \texttt{workflowVariants()} function as follows:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{workflowVariants}\hlstd{(}\hlstr{'myWorkFlow'}\hlstd{,}\hlkwc{x}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{5}\hlstd{,}\hlnum{7}\hlstd{),}\hlkwc{y}\hlstd{=}\hlkwd{c}\hlstd{(T,F),}\hlkwc{as.is}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{'x'}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


While the previous call would generate 8 variants, this one only generates 2.

Let us see a concrete example of a user supplied workflow
function. Imagine we want to evaluate a kind of ensemble model formed
by a regression tree and a multiple linear regression model on an
algae blooms data set~\cite{Tor10}. Moreover, let us suppose we are
interested in using the correlation between the predictions and true
values as evaluation metric. We could start by writing the following
workflow function that implements our modelling approach:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{RLensemble} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{f}\hlstd{,}\hlkwc{tr}\hlstd{,}\hlkwc{ts}\hlstd{,}\hlkwc{weightRT}\hlstd{=}\hlnum{0.5}\hlstd{,}\hlkwc{step}\hlstd{=}\hlnum{FALSE}\hlstd{,}\hlkwc{...}\hlstd{) \{}
  \hlkwd{require}\hlstd{(DMwR,}\hlkwc{quietly}\hlstd{=F)}
  \hlstd{noNAsTR} \hlkwb{<-} \hlkwd{knnImputation}\hlstd{(tr)}
  \hlstd{noNAsTS} \hlkwb{<-} \hlkwd{knnImputation}\hlstd{(ts)}
  \hlstd{r} \hlkwb{<-} \hlkwd{rpartXse}\hlstd{(f,tr,...)}
  \hlstd{l} \hlkwb{<-} \hlkwd{lm}\hlstd{(f,noNAsTR)}
  \hlkwa{if} \hlstd{(step) l} \hlkwb{<-} \hlkwd{step}\hlstd{(l,}\hlkwc{trace}\hlstd{=}\hlnum{0}\hlstd{)}
  \hlstd{pr} \hlkwb{<-} \hlkwd{predict}\hlstd{(r,ts)}
  \hlstd{pl} \hlkwb{<-} \hlkwd{predict}\hlstd{(l,noNAsTS)}
  \hlstd{ps} \hlkwb{<-} \hlstd{weightRT}\hlopt{*}\hlstd{pr}\hlopt{+}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{weightRT)}\hlopt{*}\hlstd{pl}
  \hlkwd{WFoutput}\hlstd{(}\hlkwd{c}\hlstd{(}\hlkwc{correlation}\hlstd{=}\hlkwd{cor}\hlstd{(}\hlkwd{responseValues}\hlstd{(f,ts),ps)))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


This workflow starts by building two modified samples of the training
and testing sets, with the \texttt{NA} values being filled in using a
nearest neighbour strategy (see the help page of the function
\texttt{knnImputation()} of package \textbf{DMwR}~\cite{Tor10} for more
details). These versions are to be used by the \texttt{lm()} function
that is unable to cope with cases with missing values. After obtaining
the two models and their predictions the function calculates a
weighted average of both predictions before obtaining and returning
the respective correlation score.

To evaluate different variants of this workflow we could run the
following experiment:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(algae,}\hlkwc{package}\hlstd{=}\hlstr{'DMwR'}\hlstd{)}
\hlstd{expRes} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(a1} \hlopt{~} \hlstd{.,algae[,}\hlnum{1}\hlopt{:}\hlnum{12}\hlstd{],}\hlstr{'alga1'}\hlstd{),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'RLensemble'}\hlstd{,}
                  \hlkwc{se}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{),}\hlkwc{step}\hlstd{=}\hlkwd{c}\hlstd{(T,F),}\hlkwc{weightRT}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.4}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlnum{0.6}\hlstd{)),}
  \hlkwd{BootSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{100}\hlstd{,}\hlkwc{type}\hlstd{=}\hlstr{'e0'}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


In this experimental comparison we have used 100 repetitions of a $\epsilon_0$ 
bootstrap estimation procedure as estimation methodology (further
details on this and other methodologies will be given later), to
compare 12 variants of our workflow. 

\subsection{Generic Workflows}

Writing workflow functions may be tedious on large comparisons,
particularly when few details change among them. Moreover, the most
frequent use of our infra-structure will probably be to compare
existing modelling techniques (or variants of them) on one or more problems. This means that
the most frequently used workflows will essentially build a model
using some existing algorithm, obtain its predictions and then
calculate some standard prediction error metrics. To facilitate this type of tasks, we
also provide generic workflow functions that carry out this type
of process for any modelling technique. The idea is to save the user
from having to write these functions provided his/her workflow fits
this generic schema.

\subsubsection{Classification and Regression Tasks}

Function \texttt{standardWF()} implements a typical workflow for both
classification and regression tasks. Apart from a formula, a training set data frame and a test set data frame, this function has the following parameters that help the user to specify is intended workflow:

\begin{description}
\item[learner] - the name of a R function that obtains a model from the training data. This function will be called with a formula in the first argument and the training set data frame in the second.
\item[learner.pars] - a list specifying any extra parameter settings that should be added to the formula and training set, at the time the learner function is called (defaults to \texttt{NULL}).
\item[predictor] - the name of a R function that is able to obtain the predictions of the model obtained with \texttt{learner}. This function will be called with the object resulting from the \texttt{learner} call on the first argument and the test set data frame in the second (it defaults to function \texttt{predict()}).
\item[predictor.pars] - a list specifying any extra parameter settings that should be added to the model and test set, at the time the predictor function is called (defaults to \texttt{NULL}).
\item[evaluator] - the name of a R function that is able to calculate the evaluation metrics you want to estimate based on the predictions of the model and the true values of the target variable on the given test set (it will default to \texttt{classificationMetrics()} function if it is a classification task, and to \texttt{regressionMetrics()} if a regression task - check the respective help pages to see what are the default metrics that are calculated for each). This function will be called with the values of the target variable in the test set on the first argument and with the result of the call to the predictor function on the second.
\item[evaluator.pars] -  a list specifying any extra parameter settings that should be added to the true values and predictions, at the time the evaluator function is called (defaults to \texttt{NULL}). A typical usage here would be to use the parameter \texttt{stats} of functions \texttt{classificationMetrics()} and \texttt{regressionMetrics()} to specify the metrics you want to calculate, provided you are using these functions as evaluators.
\end{description}


Below you find an example of one of the most frequent type of
comparisons users carry out - checking which is the ``best'' model for
a given predictive task. Let us restrict the search to a small set of
models for illustrative purposes and let us play with the well-known
Boston housing regression task:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(Boston,}\hlkwc{package}\hlstd{=}\hlstr{'MASS'}\hlstd{)}
\hlkwd{library}\hlstd{(DMwR)}
\hlkwd{library}\hlstd{(e1071)}
\hlkwd{library}\hlstd{(randomForest)}
\hlstd{bostonRes} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(medv} \hlopt{~} \hlstd{.,Boston),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
                   \hlkwc{learner}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{'rpartXse'}\hlstd{,}\hlstr{'svm'}\hlstd{,}\hlstr{'randomForest'}\hlstd{)),}
  \hlkwd{CvSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{1}\hlstd{,}\hlkwc{nFolds}\hlstd{=}\hlnum{10}\hlstd{)}
  \hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


Notice that on this simple example we have used all modelling tools
with their default parameter settings which is not necessarily a good
idea when we are looking for the best performance. Still, the goal of
this illustration is to show you how simple this type of experiments
can be if you are using a standard workflow setting. In case you want
to use the modelling tools with other parameter settings then you
should separate them in different \texttt{workflowVariants()} calls, as shown
in the following example:

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(Boston,}\hlkwc{package}\hlstd{=}\hlstr{'MASS'}\hlstd{)}
\hlkwd{library}\hlstd{(DMwR)}
\hlkwd{library}\hlstd{(e1071)}
\hlkwd{library}\hlstd{(randomForest)}
\hlstd{bostonRes} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(medv} \hlopt{~} \hlstd{.,Boston),}
  \hlkwd{c}\hlstd{(}\hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}\hlkwc{learner}\hlstd{=}\hlstr{'rpartXse'}\hlstd{,}
                     \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{se}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{))),}
    \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}\hlkwc{learner}\hlstd{=}\hlstr{'svm'}\hlstd{,}
                     \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{cost}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{5}\hlstd{),}\hlkwc{gamma}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.01}\hlstd{,}\hlnum{0.1}\hlstd{))),}
    \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}\hlkwc{learner}\hlstd{=}\hlstr{'randomForest'}\hlstd{,}
                     \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{ntree}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{500}\hlstd{,}\hlnum{1000}\hlstd{)))),}
  \hlkwd{CvSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{1}\hlstd{,}\hlkwc{nFolds}\hlstd{=}\hlnum{10}\hlstd{)}
  \hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


Notice that this code involves estimating the mean squared error on the Boston Housing task for 8 different models through
10-fold cross validation. 


\subsubsection{Time Series Tasks}

Our infra-structure also includes another generic workflow function
that is specific for predictive tasks with time-dependent data
(e.g. time series forecasting problems). This workflow function
implements two different approaches to the problem of training a
 model with a set of time-dependent data and then use it to
obtain predictions for a test set in the future. These two approaches
contrast with the standard approach of learning a model with the
available training sample and then use it to obtain predictions for
all test period. This standard approach could be applied using the previously
described \texttt{standardWF()} function. However, there are
alternatives to this procedure, two of the most common being the
sliding and growing window approaches, which are implemented in another workflow function developed specifically for time series tasks.

Predictive tasks for time-dependent data are different from standard
classification and regression tasks because they require that the test
samples have time stamps that are more recent then training
samples. In this context, experimental methodologies handling these
tasks typically do not shuffle the observations to maintain the time ordering
in the original data. The most common setup is that we have a $L$  time steps
training window containing observations in the period $[t_1,t_L]$ and a $F$
time steps test window typically containing the observations in the
time window $[t_{L+1},t_{L+F}]$. 

The idea of the
sliding window method is that if we want a prediction for time point
$t_k$ belonging to the test interval $[t_{L+1},t_{L+F}]$ then we can
assume that all data from $t_{L+1}$ till $t_{k-1}$ is already past,
and thus usable by the model. In this context, it may be wise to use
this new data in the interval $[t_{L+1},t_{k-1}]$ to update the
original model obtained using only the initial training period data. This is
particularly advisable if we suspect that the conditions may have
changed since the training period has ended. Model updating using the
sliding window method is carried out by using the data in the $L$ last
time steps, i.e. every new model is always obtained using the last $L$
data observations, as if the training window was slided forward in
time. Our \texttt{timeseriesWF()} function implements this idea for
both time series with a numeric target variable and a nominal target
variable. This function has a parameter (\texttt{type}) that if set to
``slide'' will use a sliding window approach. As with the
\texttt{standardWF()} function, this \texttt{timeseriesWF()} function
also accepts parameters specifying the learner, predictor, evaluator
and their respective parameters. Moreover, this function also includes
an extra parameter, named \texttt{relearn.step}, which allows the user
to establish the frequency of model updating. By default this is every
new test sample, i.e. $1$, but the user may set a less frequent
model-updating policy by using higher values of this parameter.  

The
idea of the growing window approach is very similar. The only difference
is on the data used when updating the models. Whilst sliding window
uses the data occurring in the last $L$ time steps, growing window
keeps increasing the original training window with the newly available
data points, i.e. the models are obtained with increasingly larger
training samples. By setting the parameter \texttt{type} to ``grow''
you get the \texttt{timseriesWF()} function to use this method.


% ====================================================================
\section{Estimation Methodologies}\label{sec:expMeth}

There are different ways of providing reliable estimates of the
predictive performance of a workflow. Our infra-structure implements some
of the most common estimation methods. In this section we
briefly describe them and provide short illustrative examples of their
use.

\subsection{Cross Validation}

$k$-Fold cross validation (CV) is one of the most common 
methods to estimate the predictive performance of a model. By
including an S4 object of class \textbf{CvSettings} in the third
argument of function \texttt{performanceEstimation()} we can carry
out experiments of this type.

The function \texttt{CvSettings()} can be used as a constructor of objects
of class \textbf{CvSettings}. It accepts the following parameters:

\begin{description}
\item[nReps] - the number of repetitions of the $k$-fold CV experiment (default is $1$)
\item[nFolds] - the number of $k$ folds to use (default is $10$)
\item[seed] - the random number generator seed to use (default is $1234$)
\item[strat] - whether to use stratified samples (default is \texttt{FALSE})
\item[dataSplits] - a list containing user-supplied data splits
  for each of the folds and repetitions (check the help page of the
  class for further details). This parameter defaults to
  \texttt{NULL}, i.e. no user-supplied splits, they are decided
  internally by the infra-structure.
\end{description}

Bellow you can find a small illustration using the Breast Cancer data
set available in package \textbf{mlbench}. On this example we compare
some variants of an SVM using a $3\times 10-$fold cross validation
process with stratified sampling because one of the two classes has a
considerably lower frequency.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(BreastCancer,}\hlkwc{package}\hlstd{=}\hlstr{'mlbench'}\hlstd{)}
\hlkwd{library}\hlstd{(e1071)}
\hlkwd{library}\hlstd{(DMwR)}
\hlstd{bc} \hlkwb{<-} \hlkwd{knnImputation}\hlstd{(BreastCancer[,}\hlopt{-}\hlnum{1}\hlstd{])}
\hlstd{bcExp} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(Class} \hlopt{~} \hlstd{.,bc,}\hlstr{'BreastCancer'}\hlstd{),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
           \hlkwc{learner}\hlstd{=}\hlstr{'svm'}\hlstd{,}
           \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{cost}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{5}\hlstd{),}\hlkwc{gamma}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.01}\hlstd{,}\hlnum{0.1}\hlstd{)),}
           \hlkwc{evaluator.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{stats}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"F"}\hlstd{,}\hlstr{"prec"}\hlstd{,}\hlstr{"rec"}\hlstd{),}
                               \hlkwc{posClass}\hlstd{=}\hlstr{"malignant"}\hlstd{)}
          \hlstd{),}
  \hlkwd{CvSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{3}\hlstd{,}\hlkwc{nFolds}\hlstd{=}\hlnum{10}\hlstd{,}\hlkwc{strat}\hlstd{=}\hlnum{TRUE}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


Please note the use of the \texttt{evaluator.pars} parameter of the
\texttt{standardWF()} function. We have used it to indicate several
settings to be used in the call to the evaluation function, which by
default is \texttt{classificationMetrics} in the case of
classification problems when using our standard workflow. In this case
we are specifying some of the available classification metrics and
indicating which of the two classes of the problem is to be considered
the positive class\footnote{The F-measure, Recall and Precision
  metrics are calculated for two-class problems with respect to one of
  the classes, usually named the ``positive'' class.}.

\subsection{Bootstrapping}

Bootstrapping or bootstrap resampling is another well-known
experimental methodology that is implemented in our
infra-structure. Namely, we implement two of the most common methods of obtaining bootstrap estimates: $\epsilon_0$ and $.632$ bootstrap.
By including an S4 object of class
\textbf{BootSettings} in the third argument of function
\texttt{performanceEstimation()} we can carry out experiments of this
type.

Function \texttt{BootSettings()} can be used as a constructor of
objects of class \textbf{BootSettings}. It accepts the following
arguments:

\begin{description}
\item[type] - a string with the type of bootstrap estimates: either "e0" for $\epsilon_0$ bootstrap, or ".632" for $.632$ bootstrap (default is "e0")
\item[nReps] - the number of repetitions of the bootstrap experiment (default is $200$)
\item[seed] - the random number generator seed to use (default is $1234$)
\item[dataSplits] - a list containing user-supplied data splits
  for each of the repetitions (check the help page of the
  class for further details). This parameter defaults to
  \texttt{NULL}, i.e. no user-supplied splits, they are decided
  internally by the infra-structure.
\end{description}

Bellow you can find a small illustration using the Servo data set available in package \textbf{mlbench}. On this example we compare some variants of an artificial neural network using 100 repetitions of a bootstrap experiment. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(Servo,}\hlkwc{package}\hlstd{=}\hlstr{'mlbench'}\hlstd{)}
\hlkwd{library}\hlstd{(nnet)}
\hlstd{nnExp} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(Class} \hlopt{~} \hlstd{.,Servo),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
           \hlkwc{learner}\hlstd{=}\hlstr{'nnet'}\hlstd{,}
           \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{trace}\hlstd{=F,}\hlkwc{linout}\hlstd{=T,}
               \hlkwc{size}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{5}\hlstd{),}\hlkwc{decay}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.01}\hlstd{,}\hlnum{0.1}\hlstd{))}
          \hlstd{),}
  \hlkwd{BootSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{100}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


\subsection{Holdout and Random Subsampling}

The Holdout is another frequently used experimental methodology,
particularly for large data sets. To carry out this type of
experiments in our infra-structure we can include an S4 object of
class \textbf{HldSettings} in the third argument of function
\texttt{performanceEstimation()}.

Function \texttt{HldSettings()} can be used as a constructor of
objects of class \textbf{HldSettings}. It accepts the following
arguments:

\begin{description}
\item[nReps] - the number of repetitions of the Holdout experiment (default is $1$)
\item[hldSz] - the percentage  of cases (a number between 0 and 1) to leave as holdout (test set) (default is $0.3$)
\item[seed] - the random number generator seed to use (default is $1234$)
\item[strat] - whether to use stratified samples (default is \texttt{FALSE})
\item[dataSplits] - a list containing user-supplied data splits
  for each of the repetitions (check the help page of the
  class for further details). This parameter defaults to
  \texttt{NULL}, i.e. no user-supplied splits, they are decided
  internally by the infra-structure.
\end{description}

Please note that for the usual meaning of Holdout the number of repetitions should be 1 (the default), while larger values of this parameter correspond to what is usually known as random subsampling.

The following is a small illustrative example of the use of the
random subsampling with the LetterRecognition classification task from package
\textbf{mlbench}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(LetterRecognition,}\hlkwc{package}\hlstd{=}\hlstr{'mlbench'}\hlstd{)}
\hlstd{ltrExp} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(lettr} \hlopt{~} \hlstd{.,LetterRecognition),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
           \hlkwc{learner}\hlstd{=}\hlstr{'rpartXse'}\hlstd{,}
           \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{se}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{)),}
           \hlkwc{predictor.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{type}\hlstd{=}\hlstr{'class'}\hlstd{)}
           \hlstd{),}
  \hlkwd{HldSettings}\hlstd{(}\hlkwc{nReps}\hlstd{=}\hlnum{3}\hlstd{,}\hlkwc{hldSz}\hlstd{=}\hlnum{0.3}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


Please note the use of the \texttt{predictor.pars} parameter of our
\texttt{standardWF()} function to be able to cope with the fact that
the \texttt{predict} method for classification trees requires the use
of \texttt{type="class"} to get actual predicted class labels instead
of class probabilities.

\subsection{Leave One Out Cross Validation}

Leave one out cross validation is a type of cross validation method
that is mostly used for small data sets. You can think of leave one
out cross validation as a $k$-fold cross validation with $k$ equal to
the size of the available data set. To carry out this type of
experiments in our infra-structure we can include an S4 object of
class \textbf{LoocvSettings} in the third argument of function
\texttt{performanceEstimation()}.

Function \texttt{LoocvSettings()} can be used as a constructor of
objects of class \textbf{LoocvSettings}. It accepts the following
arguments:

\begin{description}
\item[seed] - the random number generator seed to use (default is $1234$)
\item[verbose] - whether the execution of the experiments should provide a verbose form of output (default is \texttt{FALSE})
\end{description}


The following is a small illustrative example of the use of the
Holdout with the LetterRecognition classification task from package
\textbf{mlbench}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{data}\hlstd{(iris)}
\hlkwd{library}\hlstd{(e1071)}
\hlstd{irisExp} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(Species} \hlopt{~} \hlstd{.,iris),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
           \hlkwc{learner}\hlstd{=}\hlstr{'svm'}\hlstd{,}
           \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{cost}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{10}\hlstd{))}
           \hlstd{),}
  \hlkwd{LoocvSettings}\hlstd{())}
\end{alltt}
\end{kframe}
\end{knitrout}



\subsection{Monte Carlo Experiments}

Monte Carlo experiments are similar to random subsampling (or repeated
Holdout) in the sense that they consist of repeating a learning +
testing cycle several times using different data samples. The main
different lies on the way the samples are obtained. In Monte Carlo
experiments the original order of the observations is respected and
train and test splits are obtained such that the testing samples
appear ``after'' the training samples, thus being the methodology of
choice when you are comparing time series forecasting models. The idea
of Monte Carlo experiments is the following: (i) given a data set
spanning from time $t_1$ till time $t_N$, (ii) given a training set
time interval size $L$ and a test set time interval size $F$, such
that $T+F < N$, (iii) Monte Carlo experiments generate $R$ random time
points from the interval $[t_{1+T},t_{N-F}]$, and then (iv) for each
of these $R$ time points they generate a training set with data in the
interval $[t_{R-T+1},t_{R}]$ and a test set with data in the interval
$[t_{R+1},t_{R+F}]$. Using this process $R$ train+test cycles are
carried out using the user-supplied workflow function, and the
experiment estimates result from the average of the $R$ scores as
usual.

To carry out this type of experiments in our infra-structure we can
include an S4 object of class \textbf{McSettings} in the third
argument of function \texttt{performanceEstimation()}.

The function \texttt{McSettings()} can be used as a constructor of
objects of class \textbf{McSettings}. It accepts the following
arguments:

\begin{description}
\item[nReps] - the number of repetitions of the Monte Carlo experiment (default is $10$)
\item[szTrain] - the percentage (a number between 0 and 1) or the actual number of cases to use in the training samples (default is $0.25$)
\item[szTest] - the percentage (a number between 0 and 1) or the actual  number of cases to use in the test samples (default is $0.25$)
\item[seed] - the random number generator seed to use (default is $1234$)
\item[dataSplits] - a list containing user-supplied data splits
  for each of the repetitions (check the help page of the
  class for further details). This parameter defaults to
  \texttt{NULL}, i.e. no user-supplied splits, they are decided
  internally by the infra-structure.
\end{description}

The following is a small illustrative example using the quotes of the
SP500 index. This example compares two random forests with 500
regression trees, one applied in a standard way, and the other using
a sliding window with a relearn step of 5 days. The experiment
uses 10 repetitions of a train+test cycle using 50\% of the available
data for training and 25\% for testing.

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(quantmod)}
\hlkwd{library}\hlstd{(randomForest)}
\hlkwd{getSymbols}\hlstd{(}\hlstr{'^GSPC'}\hlstd{,}\hlkwc{from}\hlstd{=}\hlstr{'2008-01-01'}\hlstd{,}\hlkwc{to}\hlstd{=}\hlstr{'2012-12-31'}\hlstd{)}
\hlstd{data.model} \hlkwb{<-} \hlkwd{specifyModel}\hlstd{(}
  \hlkwd{Next}\hlstd{(}\hlnum{100}\hlopt{*}\hlkwd{Delt}\hlstd{(}\hlkwd{Ad}\hlstd{(GSPC)))} \hlopt{~} \hlkwd{Delt}\hlstd{(}\hlkwd{Ad}\hlstd{(GSPC),}\hlkwc{k}\hlstd{=}\hlnum{1}\hlopt{:}\hlnum{10}\hlstd{)}\hlopt{+}\hlkwd{Delt}\hlstd{(}\hlkwd{Vo}\hlstd{(GSPC),}\hlkwc{k}\hlstd{=}\hlnum{1}\hlopt{:}\hlnum{3}\hlstd{))}
\hlstd{data} \hlkwb{<-} \hlkwd{modelData}\hlstd{(data.model)}
\hlkwd{colnames}\hlstd{(data)[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlstr{'PercVarClose'}
\hlstd{spExp} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(PercVarClose} \hlopt{~} \hlstd{.,data,}\hlstr{'SP500_2012'}\hlstd{),}
  \hlkwd{c}\hlstd{(}\hlkwd{Workflow}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}\hlkwc{wfID}\hlstd{=}\hlstr{"standRF"}\hlstd{,}
             \hlkwc{learner}\hlstd{=}\hlstr{'randomForest'}\hlstd{,}\hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{ntree}\hlstd{=}\hlnum{500}\hlstd{)),}
    \hlkwd{Workflow}\hlstd{(}\hlstr{'timeseriesWF'}\hlstd{,efID,}\hlstr{"slideRF"}\hlstd{,}
             \hlkwc{learner}\hlstd{=}\hlstr{'randomForest'}\hlstd{,}
             \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{ntree}\hlstd{=}\hlnum{500}\hlstd{,}\hlkwc{relearn.step}\hlstd{=}\hlnum{5}\hlstd{))}
   \hlstd{),}
  \hlkwd{McSettings}\hlstd{(}\hlnum{10}\hlstd{,}\hlnum{0.5}\hlstd{,}\hlnum{0.25}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


Note that in the above example we have not tried any variants of the two workflows that are applied to the task. This means that we have used directly the \texttt{Workflow} constructor to create our workflow. Note also the use of the \texttt{wfID} parameter of this constructor to allow you to give a particular workflow ID to some approach.

% ====================================================================
\section{Statistical Significance of Differences}

The estimation methodologies that we have presented in the previous
section allow the user to obtain estimates of the mean predictive
performance of different workflows or variants of these workflows, on
different predictive tasks. We have seen that by applying the
\texttt{summary} method to the objects resulting from the estimation experiments
we can obtain the average performance for each candidate workflow on
each task. These numbers are estimates of the expected mean
performance of the workflows on the respective tasks. Being estimates,
the obvious next question is to check whether the observed differences
in performance between the workflows are statistically
significant. More formally, we want to know that confidence level of rejecting the null hypothesis that the difference between the estimated averages is zero.

That is the goal of the function
\texttt{pairedComparisons()}. This function carries out a series of pairwise
comparisons between a selected baseline workflow and a set of alternatives. The goal of each of these paired comparisons is to check the confidence we have in rejecting the null hypothesis that the difference between the mean performance of the baseline workflow and that of the alternatives is zero. These paired comparisons are carried out for all predictive tasks included in the estimation process.

Our experimental infra-structure ensures that all compared workflows
are run on exactly the same train+test partitions on all repetitions and
for all predictive tasks. In this context, we can focus on pairwise
statistical significance tests. Given that we cannot ensure that the
different iterations are statistically independent (for instance there
may be some overlap between the training samples)\footnote{That independence can only be ensured  for a single repetition of Holdout.}, we use the Wilcoxon
signed rank test to assess the statistical significance of the
differences between every pair of compared workflows. Let us see a
concrete example:




\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(DMwR)}
\hlkwd{data}\hlstd{(LetterRecognition,}\hlkwc{package}\hlstd{=}\hlstr{'mlbench'}\hlstd{)}
\hlstd{ltrExp} \hlkwb{<-} \hlkwd{performanceEstimation}\hlstd{(}
  \hlkwd{PredTask}\hlstd{(lettr} \hlopt{~} \hlstd{.,LetterRecognition),}
  \hlkwd{workflowVariants}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
           \hlkwc{learner}\hlstd{=}\hlstr{'rpartXse'}\hlstd{,}
           \hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{se}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{)),}
           \hlkwc{predictor.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{type}\hlstd{=}\hlstr{'class'}\hlstd{)}
           \hlstd{),}
  \hlkwd{HldSettings}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{0.3}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}


Using the \texttt{topPerformers()} function we can find out the best
scoring variant of this comparison of rpartXse-based workflows,

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{topPerformers}\hlstd{(ltrExp)}
\end{alltt}
\begin{verbatim}
## $LetterRecognition
##        Workflow Estimate
## err rpartXse.v1    0.143
\end{verbatim}
\end{kframe}
\end{knitrout}


Now we can proceed to check whether the advantage of this variant over
the others is statistically significant,

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pairedComparisons}\hlstd{(ltrExp,}\hlstr{'rpartXse.v1'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## , , err, LetterRecognition
## 
##             AvgScore  SdScore      Diff p.value
## rpartXse.v1   0.1434 0.005453        NA      NA
## rpartXse.v2   0.1486 0.011612 -0.005222    0.25
\end{verbatim}
\end{kframe}
\end{knitrout}


The function \texttt{pairedComparisons()} receives as first argument the
object resulting from the comparative experiments. The second argument
is the baseline workflow against which you want to compare the others
to. The output of this function is array with four dimensions. The first dimension contains the different workflows involved on each paired comparison, with the baseline in the first row (and with the columns ``Diff'' and ``p.value'' set to \texttt{NA}). The second dimension includes the information provided for each workflow, namely: i) the average estimated score for the metric; ii) the respective standard error of this estimate; iii) the difference between the score of the baseline and the score of each alternative; and iv) the confidence level (p-value) for rejecting the null hypothesis that the difference is zero.  The third dimension of the array are the metrics involved in the estimation experiment (in the above example it is a single one - the error rate), whilst the fourth dimension is the predictive tasks (again the above example only includes a single task).

In the above example the p-value for the single paired comparison that is carried out by the function is too high (0.25) and thus we can not reject the hypothesis that the difference between the error rate of two alternative workflows is zero in this particular predictive task.



% ====================================================================
\section{Larger Examples}

The main advantage of the infra-structure we are proposing is to
automate large scale performance estimation experiments. It is on these very
large setups that the use of the infra-structure spares more time to
the user. However, in these contexts the objects resulting from the
estimation process are very large and some of the tools we have shown before
for exploring the results may produce over-cluttered output. In
effect, if you have an experiment involving dozens of predictive tasks
and eventually hundreds of workflow variants being compared on several
evaluation metrics, doing a plot of the resulting object is simply not
possible as the graph will be unreadable. This section illustrates
some of these cases and presents some solutions to overcome the
difficulties they bring.

Extremely large experiments may take days or weeks to complete,
depending on the available hardware. In this context, it may not be
wise to run the experiments on a single call to the
\texttt{performanceEstimation()} function because if something goes
wrong in the middle you may loose lots of work/time. Using the random
number generation seeds that are available in all experimental
settings objects we can split the experiments in several calls and
still ensure that the same data folds are used in all
estimation experiments. Moreover, we will see that when all experiments are
finished we will be able to merge the objects of each call into a
single object as if we had issued a single call. Let us see an
example.

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(DMwR)}
\hlkwd{library}\hlstd{(e1071)}
\hlkwd{library}\hlstd{(randomForest)}

\hlkwd{data}\hlstd{(algae)}
\hlstd{DSs} \hlkwb{<-} \hlkwd{sapply}\hlstd{(}\hlkwd{names}\hlstd{(algae)[}\hlnum{12}\hlopt{:}\hlnum{18}\hlstd{],}
         \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,}\hlkwc{names.attrs}\hlstd{) \{}
           \hlstd{f} \hlkwb{<-} \hlkwd{as.formula}\hlstd{(}\hlkwd{paste}\hlstd{(x,}\hlstr{"~ ."}\hlstd{))}
           \hlkwd{PredTask}\hlstd{(f,algae[,}\hlkwd{c}\hlstd{(names.attrs,x)],x)}
         \hlstd{\},}
         \hlkwd{names}\hlstd{(algae)[}\hlnum{1}\hlopt{:}\hlnum{11}\hlstd{])}

\hlstd{WFs} \hlkwb{<-} \hlkwd{list}\hlstd{()}
\hlstd{WFs}\hlopt{$}\hlstd{svm} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{cost}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{10}\hlstd{,}\hlnum{150}\hlstd{,}\hlnum{300}\hlstd{),}
                    \hlkwc{gamma}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.01}\hlstd{,}\hlnum{0.001}\hlstd{),}\hlkwc{epsilon}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.1}\hlstd{,}\hlnum{0.05}\hlstd{)))}
\hlstd{WFs}\hlopt{$}\hlstd{randomForest} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{learner.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{mtry}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{5}\hlstd{,}\hlnum{7}\hlstd{),}
                             \hlkwc{ntree}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{500}\hlstd{,}\hlnum{750}\hlstd{,}\hlnum{1500}\hlstd{)))}

\hlkwa{for}\hlstd{(d} \hlkwa{in} \hlkwd{seq_along}\hlstd{(DSs)) \{}
  \hlkwa{for}\hlstd{(w} \hlkwa{in} \hlkwd{names}\hlstd{(WFs)) \{}
    \hlstd{resObj} \hlkwb{<-} \hlkwd{paste}\hlstd{(}\hlkwd{names}\hlstd{(DSs)[d],w,}\hlstr{'Res'}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{''}\hlstd{)}
    \hlkwd{assign}\hlstd{(resObj,}
           \hlkwd{performanceEstimation}\hlstd{(}
                  \hlstd{DSs[d],}
                  \hlkwd{c}\hlstd{(}
                    \hlkwd{do.call}\hlstd{(}\hlstr{'workflowVariants'}\hlstd{,}
                            \hlkwd{c}\hlstd{(}\hlkwd{list}\hlstd{(}\hlstr{'standardWF'}\hlstd{,}
                               \hlkwc{learner}\hlstd{=w,}
                               \hlkwc{evaluator.pars}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{stats}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{'mse'}\hlstd{,}\hlstr{'mae'}\hlstd{))),}
                              \hlstd{WFs[[w]]))}
                    \hlstd{),}
                   \hlkwd{CvSettings}\hlstd{(}\hlnum{3}\hlstd{,}\hlnum{10}\hlstd{,}\hlnum{1234}\hlstd{))}
           \hlstd{)}

    \hlkwd{save}\hlstd{(}\hlkwc{list}\hlstd{=resObj,}\hlkwc{file}\hlstd{=}\hlkwd{paste}\hlstd{(}\hlkwd{names}\hlstd{(DSs)[d],w,}\hlstr{'Rdata'}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{'.'}\hlstd{))}
  \hlstd{\}}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


The above code compares 12 SVM variants with 6 random forest variants, on 7 algae blooms regression tasks, using
$3\times 10-$fold cross validation. Although this is not a very large
experimental comparison it still includes applying 18 different
workflow variants on 7 different prediction tasks, 30 times, i.e. 3780
train+test cycles. Instead of running all these experiments on a
single call to the function \texttt{performanceEstimation()} (which
would obviously still be possible), we have made different calls for
each workflow type (SVM, random forest and MARS) and for each
predictive task. This means that each call will run all variants of a
certain workflow on a certain predictive task. The result of each of
these calls will be assigned to an object with a name composed of the
task name and workflow learner (the \texttt{resObj} variable). In the end each of these objects is saved
on a file with a similar name, for future loading and results
analysis. For instance, in the end there will be a file with name
``a1.svm.Rdata'' which contains an object of class \textbf{ComparisonResults}
named \texttt{a1svmRes}. This object contains the  MSE and MAE 
estimated scores of the SVM variants on the task of predicting the
target variable ``a1'' (one of the eight algae in this data set).

Later on, after the above experiment has finished you can load the saved objects back 
into R and moreover, join them into a single object, as shown below:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{nD} \hlkwb{<-} \hlkwd{paste}\hlstd{(}\hlstr{'a'}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{7}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{''}\hlstd{)}
\hlstd{nL} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{'svm'}\hlstd{,}\hlstr{'randomForest'}\hlstd{)}
\hlstd{res} \hlkwb{<-} \hlkwa{NULL}
\hlkwa{for}\hlstd{(d} \hlkwa{in} \hlstd{nD) \{}
  \hlstd{resD} \hlkwb{<-} \hlkwa{NULL}
  \hlkwa{for}\hlstd{(l} \hlkwa{in} \hlstd{nL) \{}
    \hlkwd{load}\hlstd{(}\hlkwd{paste}\hlstd{(d,l,}\hlstr{'Rdata'}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{'.'}\hlstd{))}
    \hlstd{x} \hlkwb{<-} \hlkwd{get}\hlstd{(}\hlkwd{paste}\hlstd{(d,l,}\hlstr{'Res'}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{''}\hlstd{))}
    \hlstd{resD} \hlkwb{<-} \hlkwa{if} \hlstd{(}\hlkwd{is.null}\hlstd{(resD)) x} \hlkwa{else} \hlkwd{mergeEstimationRes}\hlstd{(resD,x,}\hlkwc{by}\hlstd{=}\hlstr{'workflows'}\hlstd{)}
  \hlstd{\}}
  \hlstd{res} \hlkwb{<-} \hlkwa{if} \hlstd{(}\hlkwd{is.null}\hlstd{(res)) resD} \hlkwa{else} \hlkwd{mergeEstimationRes}\hlstd{(res,resD,}\hlkwc{by}\hlstd{=}\hlstr{'tasks'}\hlstd{)}
\hlstd{\}}
\hlkwd{save}\hlstd{(res,}\hlkwc{file}\hlstd{=}\hlstr{'allResultsAlgae.Rdata'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


The \texttt{mergeEstimationRes()}  function when applied to objects of class
\textbf{ComparisonResults} allows merging of these objects across different
dimensions. Namely, such objects have the individual scores of all
experiments spread across 4 dimensions: the iterations, the
metrics, the workflows and the tasks. The argument
\texttt{by} of the \texttt{mergeEstimationRes()} function allows you to specify how
to merge the given objects. The most common situations are: (i)
merging the results of different workflows over the same data sets -
you should use ``\texttt{by='workflows'}'', or (ii) merging the results
of the same workflows across different tasks - you should use
``\texttt{by='tasks'}''.

The following code can be used to check that the merging was OK, and
also to illustrate a few other utility functions whose purpose should
be obvious:




\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{res}
\end{alltt}
\begin{verbatim}
## 
## ==  Cross Validation Performance Estimation Experiment ==
## 
##  3 x 10 - Fold Cross Validation run with seed =  1234 
## 18  workflows
## tested on  7  predictive tasks
\end{verbatim}
\begin{alltt}
\hlkwd{taskNames}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
## [1] "a1" "a2" "a3" "a4" "a5" "a6" "a7"
\end{verbatim}
\begin{alltt}
\hlkwd{workflowNames}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
##  [1] "svm.v1"          "svm.v2"          "svm.v3"          "svm.v4"         
##  [5] "svm.v5"          "svm.v6"          "svm.v7"          "svm.v8"         
##  [9] "svm.v9"          "svm.v10"         "svm.v11"         "svm.v12"        
## [13] "randomForest.v1" "randomForest.v2" "randomForest.v3" "randomForest.v4"
## [17] "randomForest.v5" "randomForest.v6"
\end{verbatim}
\begin{alltt}
\hlkwd{metricNames}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
## [1] "mse" "mae"
\end{verbatim}
\end{kframe}
\end{knitrout}


With such large objects the most we can do is obtaining the best
scores or rankings of the workflows:

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{topPerformers}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
## $a1
##            Workflow Estimate
## mse randomForest.v5  233.379
## mae randomForest.v1    10.29
## 
## $a2
##            Workflow Estimate
## mse randomForest.v5  102.505
## mae          svm.v1    6.088
## 
## $a3
##            Workflow Estimate
## mse randomForest.v5   48.309
## mae          svm.v4    4.135
## 
## $a4
##            Workflow Estimate
## mse randomForest.v1   13.701
## mae         svm.v10    1.724
## 
## $a5
##            Workflow Estimate
## mse randomForest.v5   46.665
## mae         svm.v10    4.223
## 
## $a6
##     Workflow Estimate
## mse  svm.v12  114.123
## mae  svm.v12    5.859
## 
## $a7
##     Workflow Estimate
## mse   svm.v2   27.723
## mae   svm.v4    2.501
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{rankWorkflows}\hlstd{(res)}
\end{alltt}
\begin{verbatim}
## $a1
## $a1$mse
##          Workflow Estimate
## 1 randomForest.v5    233.4
## 2 randomForest.v1    233.7
## 3 randomForest.v3    235.0
## 4 randomForest.v2    236.4
## 5 randomForest.v4    238.1
## 
## $a1$mae
##          Workflow Estimate
## 1 randomForest.v1    10.29
## 2 randomForest.v5    10.31
## 3 randomForest.v2    10.34
## 4 randomForest.v3    10.34
## 5 randomForest.v6    10.37
## 
## 
## $a2
## $a2$mse
##          Workflow Estimate
## 1 randomForest.v5    102.5
## 2 randomForest.v1    103.5
## 3 randomForest.v3    103.8
## 4 randomForest.v6    104.2
## 5 randomForest.v2    104.8
## 
## $a2$mae
##   Workflow Estimate
## 1   svm.v1    6.088
## 2   svm.v7    6.155
## 3  svm.v10    6.204
## 4   svm.v4    6.226
## 5   svm.v5    6.248
## 
## 
## $a3
## $a3$mse
##          Workflow Estimate
## 1 randomForest.v5    48.31
## 2 randomForest.v3    48.37
## 3 randomForest.v1    48.40
## 4 randomForest.v2    49.06
## 5 randomForest.v6    49.17
## 
## $a3$mae
##   Workflow Estimate
## 1   svm.v4    4.135
## 2  svm.v10    4.155
## 3   svm.v7    4.224
## 4   svm.v1    4.248
## 5   svm.v5    4.269
## 
## 
## $a4
## $a4$mse
##          Workflow Estimate
## 1 randomForest.v1    13.70
## 2 randomForest.v3    13.85
## 3 randomForest.v5    13.87
## 4 randomForest.v4    13.99
## 5 randomForest.v6    14.05
## 
## $a4$mae
##          Workflow Estimate
## 1         svm.v10    1.724
## 2          svm.v4    1.726
## 3          svm.v7    1.769
## 4 randomForest.v3    1.783
## 5 randomForest.v1    1.788
## 
## 
## $a5
## $a5$mse
##          Workflow Estimate
## 1 randomForest.v5    46.66
## 2 randomForest.v3    46.72
## 3 randomForest.v1    46.89
## 4 randomForest.v6    47.79
## 5 randomForest.v4    47.89
## 
## $a5$mae
##   Workflow Estimate
## 1  svm.v10    4.223
## 2   svm.v4    4.252
## 3   svm.v1    4.292
## 4   svm.v7    4.305
## 5  svm.v11    4.335
## 
## 
## $a6
## $a6$mse
##          Workflow Estimate
## 1         svm.v12    114.1
## 2          svm.v6    115.9
## 3          svm.v5    122.5
## 4         svm.v11    122.9
## 5 randomForest.v3    131.7
## 
## $a6$mae
##   Workflow Estimate
## 1  svm.v12    5.859
## 2   svm.v6    5.913
## 3   svm.v1    5.950
## 4   svm.v7    5.959
## 5  svm.v11    5.978
## 
## 
## $a7
## $a7$mse
##          Workflow Estimate
## 1          svm.v2    27.72
## 2          svm.v8    27.86
## 3 randomForest.v3    28.26
## 4 randomForest.v5    28.36
## 5          svm.v1    28.50
## 
## $a7$mae
##   Workflow Estimate
## 1   svm.v4    2.501
## 2  svm.v10    2.506
## 3   svm.v1    2.513
## 4   svm.v7    2.515
## 5  svm.v11    2.530
\end{verbatim}
\end{kframe}
\end{knitrout}


Notice that both \texttt{topPerformers()} and \texttt{rankWorkflows()}
assume that the evaluation metrics are to be minimized, i.e. they
assume the lower the better the scores. Still, both functions have a
parameter named \texttt{maxs} that accepts a vector with as many
Boolean values as there are evaluation metrics being estimated, which
you may use to indicate that some particular metric is to be maximized
and not minimized (the default). So for instance, if you had an
experiment where the 1st and 3rd metrics are to be minimized, whilst
the second is to be maximized, you could call these functions as
\texttt{rankWorkflows(resObj,maxs=c(F,T,F))}.

In order to obtain further results from these large objects one
usually proceeds by analyzing parts of the object, for instance
focusing on a particular task or metric, or even a subset of the
workflows. To facilitate this we can use the generic function
\texttt{subset()} that can also be applied to objects of class
\textbf{ComparisonResults}. An example of its use is given below, which results
in a graph of the performance of the different workflows in the
predictive task ``a1'', in terms of ``MSE'', which is show in
Figure~\ref{fig:maeA1}.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{subset}\hlstd{(res,}\hlkwc{tasks}\hlstd{=}\hlstr{'a1'}\hlstd{,}\hlkwc{statistics}\hlstd{=}\hlstr{'mae'}\hlstd{))}
\end{alltt}
\end{kframe}\begin{figure}[]


{\centering \includegraphics[width=0.75\textwidth]{figures/perfEst-maeA1} 

}

\caption[The MAE results for the task ``a1'']{The MAE results for the task ``a1''.\label{fig:maeA1}}
\end{figure}


\end{knitrout}

 
As before we are using the generic function \texttt{plot()} but this
time applied to a subset of the original object with all results. This
subset is obtained using the generic function \texttt{subset()} that
accepts several parameters to specify the subset we are interested
on. In this case we are using the parameters \texttt{tasks} and
\texttt{statistics} to indicate that we want to analyze only the results
concerning the task ``a1'' and the metric ``mae''. Other possibility
is the parameter \texttt{workflows} for indicating a subset of the
workflows. Both \texttt{workflows}, \texttt{tasks} and \texttt{statistics} accept
as values a character string containing a regular expression that will
be used internally with the R function \texttt{grep()} over the vector
of names of the respective objects (names of the workflows, names of
the tasks and names of the metrics, respectively). For instance, if
you want to constrain the previous graph even further to the workflows
whose name ends in ``4'' (absurd example of course!), you could use
the following:


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwd{subset}\hlstd{(res,}\hlkwc{tasks}\hlstd{=}\hlstr{'a1'}\hlstd{,}\hlkwc{workflows}\hlstd{=}\hlstr{'4$'}\hlstd{))}
\end{alltt}
\end{kframe}\begin{figure}[]


{\centering \includegraphics[width=0.75\textwidth]{figures/perfEst-mseA1b} 

}

\caption[Illustration of the use of regular expressions in sub-setting the results objects]{Illustration of the use of regular expressions in sub-setting the results objects.\label{fig:mseA1b}}
\end{figure}


\end{knitrout}



If you are more familiar with the syntax of "wildcards" you may use
the R function \texttt{glob2rx()} to convert to regular expressions,
as show in the following example:

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(}\hlkwd{subset}\hlstd{(res,}\hlkwc{tasks}\hlstd{=}\hlstr{'a1'}\hlstd{,}\hlkwc{workflows}\hlstd{=}\hlkwd{glob2rx}\hlstd{(}\hlstr{'*svm*'}\hlstd{),}\hlkwc{statistics}\hlstd{=}\hlstr{'mse'}\hlstd{))}
\end{alltt}
\begin{verbatim}
## 
## == Summary of a  Cross Validation Performance Estimation Experiment ==
##  3 x 10 - Fold Cross Validation run with seed =  1234 
## 
## * Predictive Tasks ::  a1
## * Workflows  ::  svm.v1, svm.v2, svm.v3, svm.v4, svm.v5, svm.v6, svm.v7, svm.v8, svm.v9, svm.v10, svm.v11, svm.v12
## 
## * Summary of Experiment Results:
## 
## -> Datataset:  a1 
## 
## 	*Workflow: svm.v1 
##           mse
## avg     278.6
## std     106.5
## min     121.2
## max     503.3
## invalid   0.0
## 
## 	*Workflow: svm.v2 
##           mse
## avg     278.7
## std     107.3
## min     126.0
## max     603.8
## invalid   0.0
## 
## 	*Workflow: svm.v3 
##           mse
## avg     309.6
## std     114.8
## min     145.1
## max     659.3
## invalid   0.0
## 
## 	*Workflow: svm.v4 
##           mse
## avg     340.0
## std     138.7
## min     104.7
## max     558.5
## invalid   0.0
## 
## 	*Workflow: svm.v5 
##             mse
## avg      348.08
## std      213.27
## min       97.36
## max     1001.13
## invalid    0.00
## 
## 	*Workflow: svm.v6 
##            mse
## avg      362.3
## std      256.9
## min      106.5
## max     1268.0
## invalid    0.0
## 
## 	*Workflow: svm.v7 
##           mse
## avg     277.5
## std     104.7
## min     124.2
## max     496.0
## invalid   0.0
## 
## 	*Workflow: svm.v8 
##           mse
## avg     279.2
## std     106.9
## min     129.5
## max     609.4
## invalid   0.0
## 
## 	*Workflow: svm.v9 
##           mse
## avg     305.7
## std     114.0
## min     145.9
## max     651.8
## invalid   0.0
## 
## 	*Workflow: svm.v10 
##            mse
## avg     340.31
## std     139.09
## min      97.38
## max     562.07
## invalid   0.00
## 
## 	*Workflow: svm.v11 
##            mse
## avg      351.0
## std      211.0
## min      105.8
## max     1082.0
## invalid    0.0
## 
## 	*Workflow: svm.v12 
##            mse
## avg      350.5
## std      223.2
## min      115.5
## max     1129.8
## invalid    0.0
\end{verbatim}
\end{kframe}
\end{knitrout}



The following are some illustrations of the use of other available
utility functions.

Obtaining the scores on all iterations and metrics of a workflow on a
particular task:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{getIterationsResults}\hlstd{(res,}\hlstr{'svm.v6'}\hlstd{,}\hlstr{'a3'}\hlstd{)}
\end{alltt}
\begin{verbatim}
##           mse   mae
##  [1,]  35.167 3.738
##  [2,]  85.866 5.720
##  [3,]  24.207 3.062
##  [4,]  58.519 4.421
##  [5,] 161.427 6.916
##  [6,]  43.422 3.818
##  [7,]  77.363 5.972
##  [8,]  13.756 2.989
##  [9,]  31.301 3.522
## [10,]  31.060 4.421
## [11,]  98.894 7.058
## [12,]  34.912 3.984
## [13,]  28.457 3.358
## [14,]  51.942 4.479
## [15,] 120.784 5.356
## [16,]  18.346 3.115
## [17,]  67.551 5.057
## [18,]  78.109 4.645
## [19,]  37.893 3.866
## [20,]  23.811 3.021
## [21,]  12.475 2.573
## [22,]  32.153 3.327
## [23,]   8.395 2.284
## [24,]  72.897 5.643
## [25,]  35.598 3.699
## [26,]  73.571 5.157
## [27,] 116.683 5.329
## [28,]  68.635 4.154
## [29,]  49.770 4.765
## [30,]  57.290 4.611
\end{verbatim}
\end{kframe}
\end{knitrout}


Getting the summary of the results of a particular workflow on a  predictive task :

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{estimationSummary}\hlstd{(res,}\hlstr{'svm.v3'}\hlstd{,}\hlstr{'a7'}\hlstd{)}
\end{alltt}
\begin{verbatim}
##             mse   mae
## avg      28.632 3.060
## std      25.950 1.062
## min       4.234 1.462
## max     110.198 6.196
## invalid   0.000 0.000
\end{verbatim}
\end{kframe}
\end{knitrout}


Finally, the \texttt{metricsSummary()} function allows you to apply any
summary function (defaulting to \texttt{mean()}) to the iterations estimates. The following
calculates the median of the results of the SVMs on the task ``a1'',

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{metricsSummary}\hlstd{(}\hlkwd{subset}\hlstd{(res,}\hlkwc{workflows}\hlstd{=}\hlkwd{glob2rx}\hlstd{(}\hlstr{'*svm*'}\hlstd{),}\hlkwc{tasks}\hlstd{=}\hlstr{'a1'}\hlstd{),}
               \hlkwc{summary}\hlstd{=}\hlstr{'median'}\hlstd{)}
\end{alltt}
\begin{verbatim}
## $a1
##     svm.v1 svm.v2 svm.v3 svm.v4 svm.v5 svm.v6 svm.v7 svm.v8 svm.v9 svm.v10 svm.v11
## mse 261.78 253.78 293.82  339.9 285.49 286.73 267.68 255.11 282.80   342.4   287.8
## mae  11.34  11.97  12.99   12.3  11.66  11.76  11.34  11.92  12.89    12.3    11.9
##     svm.v12
## mse  285.40
## mae   11.82
\end{verbatim}
\end{kframe}
\end{knitrout}

%\section{Conclusions}

\bibliographystyle{alpha}
\bibliography{performanceEstimation}
\end{document}
